{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, os\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://ltdata1.informatik.uni-hamburg.de/SECOS/models_jobimtext/'\n",
    "\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text)\n",
    "\n",
    "links = list(map(lambda x: url + x.get('href'), soup.find_all('a', href=lambda x: all([x.endswith('.gz'), \"wikipedia_\" in x]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = set(list(map(lambda x: re.search(r'(?<=wikipedia_)\\w{2}(?=\\_)', x).group(0), links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_links(language):\n",
    "    candidates_filename, wordcount_filename = [l for l in links if f\"wikipedia_{language}_\" in l]\n",
    "    \n",
    "    r = requests.get(candidates_filename, stream=True)\n",
    "    with open(candidates_filename.split('/')[-1], 'wb') as f:\n",
    "        for chunk in r.raw.stream(1024, decode_content=False):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    r = requests.get(wordcount_filename, stream=True)\n",
    "    with open(wordcount_filename.split('/')[-1], 'wb') as f:\n",
    "        for chunk in r.raw.stream(1024, decode_content=False):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    \n",
    "    return candidates_filename.split('/')[-1], wordcount_filename.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(zip_file, writepath='.'):\n",
    "    with gzip.open(zip_file, 'rb') as f:\n",
    "        content = f.read().decode()\n",
    "    with open(writepath + '/' + zip_file.replace('gz', 'txt'), 'w') as f:\n",
    "        f.write(content)\n",
    "    return zip_file.replace('gz', 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'da', 'de', 'en', 'es', 'et', 'fa', 'fi', 'hu', 'la', 'lv', 'nl', 'no', 'sv'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    directory = f\"./{language}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    candidates_zipfile, wordcount_zipfile = download_links(language)\n",
    "    candidates_file = unzip_file(candidates_zipfile, f'./{language}')\n",
    "    wordcount_file = unzip_file(wordcount_zipfile, f'./{language}')\n",
    "\n",
    "    wordcount = {}\n",
    "    with open(f'./{language}/' + wordcount_file, 'r') as f:\n",
    "        for line in f:\n",
    "            word, cnt = line.strip().split('\\t')\n",
    "            if all([\n",
    "                not re.findall(r\"\\b[^\\u0000-\\u05C0\\u2100-\\u214F]+\\b\", word), # Non european characters\n",
    "                not re.findall(r\"[\\d]\", word), # No digits,\n",
    "                not re.findall(r\"[^\\w]\", word), # No non alphanum chars,\n",
    "                len(word) >= 5, # At least 5 characters long\n",
    "                int(cnt) >=3 # At least 3 observations\n",
    "                ]):\n",
    "                wordcount[word] = int(cnt)\n",
    "    \n",
    "    compounds = []\n",
    "    with open(f'./{language}/' + candidates_file, 'r') as f:\n",
    "        for line in f:\n",
    "            word = (line.split('\\t')[0]).strip()\n",
    "            if word in wordcount:\n",
    "                compounds.append(word)\n",
    "\n",
    "    with open(f'./{language}/wordcounts.json',  'w') as f:\n",
    "        json.dump(wordcount, f)\n",
    "    \n",
    "    with open(f'./{language}/compounds.txt',  'w') as f:\n",
    "        for line in compounds:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"\"\"\\\n",
    "python decompound_secos.py \\\n",
    "{language}/wikipedia_{language}_tokenized_trigram__FreqSigLMI__PruneContext_s_0.0_w_2_f_2_wf_0_wpfmax_1000_wpfmin_2_p_1000__AggrPerFt__SimCount_sc_one_ac_False__SimSortlimit_200_minsim_0_candidates.txt \\\n",
    "{language}/wikipedia_{language}_tokenized_trigram__WordCount.txt \\\n",
    "50 \\\n",
    "{language}/compounds.txt \\\n",
    "0 3 3 5 3 lower 0.01 {language}\\\n",
    "> {language}/output.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read knowledge\n",
      "extract single words\n",
      "decompound\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for language in languages:\n",
    "    print(language)\n",
    "    process = subprocess.Popen(\n",
    "        (\"conda run -n secos \" + cmd.format(language=language)).split(),stdout=subprocess.PIPE\n",
    "    )\n",
    "    output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "langmap = {\n",
    "    \"da\": \"Danish\",\n",
    "    \"de\": \"German\",\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"et\": \"Estonian\",\n",
    "    \"fa\": \"Farsi\",\n",
    "    \"fi\": \"Finish\",\n",
    "    \"hu\": \"Hungarian\",\n",
    "    \"la\": \"Latin\",\n",
    "    \"lv\": \"Latvian\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"no\": \"Norwegian\",\n",
    "    \"sv\": \"Swedish\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    \n",
    "    with open(f'./{language}/wordcounts.json', 'r') as f:\n",
    "        wordcounts = json.loads(f.read())\n",
    "    wordcounts =  {k.lower(): v for k, v in wordcounts.items()}\n",
    "\n",
    "    with open(f'./{language}/generated_dictionary.txt', 'r') as f:\n",
    "        generated_dictionary = f.read().lower().split(\"\\n\")\n",
    "    \n",
    "    with open(f'./{language}/output.txt', 'r') as f:\n",
    "        data = f.read().lower().split(\"\\n\")\n",
    "\n",
    "    precomputed_splits = {}\n",
    "    for item in data:\n",
    "        compounds = item.split(\"\\t\")[1].split('-')\n",
    "        key = item.split(\"\\t\")[-1]\n",
    "        precomputed_splits[key]=compounds\n",
    "\n",
    "    model = dict(\n",
    "           language = langmap.get(language),\n",
    "           precomputed_splits=precomputed_splits,\n",
    "           generated_dictionary=generated_dictionary,\n",
    "           word_frequencies = wordcounts,\n",
    "           total_wordcount = sum(wordcounts.values()),\n",
    "           n_words = len(wordcounts.keys())\n",
    "    )\n",
    "\n",
    "    with open(f'./{language}/{language}.json','w') as f:\n",
    "        json.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belief-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
